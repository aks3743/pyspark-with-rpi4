{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87c5cc5a-96a4-41d3-a8af-cd1425085ef1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.0.1-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (11.6 MB)\n",
      "Collecting tzdata>=2022.1\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Collecting numpy>=1.21.0\n",
      "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_aarch64.manylinux2014_aarch64.whl (14.0 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./pysparktw/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./pysparktw/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-1.24.3 pandas-2.0.1 pytz-2023.3 tzdata-2023.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install sparknlp\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b340d9a2-e687-4799-bdd0-493c9a2f7eed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea2f159-a60d-4eb3-b2e6-3e75de504b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17659/3494476965.py:1: ParserWarning: Falling back to the 'python' engine because the separator encoded in utf-8 is > 1 char long, and the 'c' engine does not support such separators; you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv('/home/abhijith/Desktop/St_apps/twitter/twitter_dump_2.csv',delimiter='¬',encoding = 'utf-8')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/home/abhijith/Desktop/St_apps/twitter/twitter_dump_2.csv',delimiter='¬',encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd4938e3-7aeb-4504-9c51-4bb49a0639b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>edit_history_tweet_ids</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>public_metrics</th>\n",
       "      <th>referenced_tweets</th>\n",
       "      <th>reply_settings</th>\n",
       "      <th>text</th>\n",
       "      <th>withheld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4832549112</td>\n",
       "      <td>1624041287300046848</td>\n",
       "      <td>2023-02-10 13:43:10+00:00</td>\n",
       "      <td>[1624041287300046848]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1624041287300046848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 264, 'reply_count': 0, 'like...</td>\n",
       "      <td>[&lt;ReferencedTweet id=1623504190738690053 type=...</td>\n",
       "      <td>everyone</td>\n",
       "      <td>RT @sukeji_handa: The Delta Snake Review: A Mu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1390342845605695489</td>\n",
       "      <td>1624041268358807552</td>\n",
       "      <td>2023-02-10 13:43:06+00:00</td>\n",
       "      <td>[1624041268358807552]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1624041268358807552</td>\n",
       "      <td>NaN</td>\n",
       "      <td>th</td>\n",
       "      <td>{'retweet_count': 8, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>[&lt;ReferencedTweet id=1624029463762866177 type=...</td>\n",
       "      <td>everyone</td>\n",
       "      <td>RT @sxicex: ถึงเวลาของ Ai-Generated Music หลัง...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1224166856040173569</td>\n",
       "      <td>1624041263518425088</td>\n",
       "      <td>2023-02-10 13:43:05+00:00</td>\n",
       "      <td>[1624041263518425088]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1624041263518425088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everyone</td>\n",
       "      <td>Why is there so much talk about a cyber attack...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1531177796495888384</td>\n",
       "      <td>1624041262398537734</td>\n",
       "      <td>2023-02-10 13:43:04+00:00</td>\n",
       "      <td>[1624041262398537734]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1624041262398537734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 418, 'reply_count': 0, 'like...</td>\n",
       "      <td>[&lt;ReferencedTweet id=1623685266480439298 type=...</td>\n",
       "      <td>everyone</td>\n",
       "      <td>RT @Connectome_CNTM: To celebrate the launch o...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>207164294</td>\n",
       "      <td>1624041258225106944</td>\n",
       "      <td>2023-02-10 13:43:03+00:00</td>\n",
       "      <td>[1624041258225106944]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1624041258225106944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 5, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>[&lt;ReferencedTweet id=1624028203307114496 type=...</td>\n",
       "      <td>everyone</td>\n",
       "      <td>RT @newscientist: ChatGPT can do more than jus...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83539</th>\n",
       "      <td>95</td>\n",
       "      <td>1184221022</td>\n",
       "      <td>1623742178194235396</td>\n",
       "      <td>2023-02-09 17:54:37+00:00</td>\n",
       "      <td>[1623742178194235396]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1623742178194235396</td>\n",
       "      <td>NaN</td>\n",
       "      <td>es</td>\n",
       "      <td>{'retweet_count': 6, 'reply_count': 1, 'like_c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everyone</td>\n",
       "      <td>ChatGPT cree los conceptos jurídicos de Lionel...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83540</th>\n",
       "      <td>96</td>\n",
       "      <td>285659105</td>\n",
       "      <td>1623742176630034435</td>\n",
       "      <td>2023-02-09 17:54:37+00:00</td>\n",
       "      <td>[1623742176630034435]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1623742176630034435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 245, 'reply_count': 0, 'like...</td>\n",
       "      <td>[&lt;ReferencedTweet id=1623313196252827650 type=...</td>\n",
       "      <td>everyone</td>\n",
       "      <td>RT @SrikantTFM: who needs ChatGPT AI when you ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83541</th>\n",
       "      <td>97</td>\n",
       "      <td>1537579497725247488</td>\n",
       "      <td>1623742170078531586</td>\n",
       "      <td>2023-02-09 17:54:35+00:00</td>\n",
       "      <td>[1623742170078531586]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1623742170078531586</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>{'retweet_count': 1312, 'reply_count': 0, 'lik...</td>\n",
       "      <td>[&lt;ReferencedTweet id=1623577688513585152 type=...</td>\n",
       "      <td>everyone</td>\n",
       "      <td>RT @nocodedevs: ChatGPT is a FREE money-genera...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83542</th>\n",
       "      <td>98</td>\n",
       "      <td>937660764428652545</td>\n",
       "      <td>1623742164831289346</td>\n",
       "      <td>2023-02-09 17:54:34+00:00</td>\n",
       "      <td>[1623742164831289346]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1623742164831289346</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zxx</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>everyone</td>\n",
       "      <td>https://t.co/NlmZpb3wa5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83543</th>\n",
       "      <td>99</td>\n",
       "      <td>1453077305014767631</td>\n",
       "      <td>1623414925421944836</td>\n",
       "      <td>2023-02-09 17:54:33+00:00</td>\n",
       "      <td>[1623742162050375687]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1623742162050375687</td>\n",
       "      <td>1.266613e+09</td>\n",
       "      <td>cs</td>\n",
       "      <td>{'retweet_count': 0, 'reply_count': 0, 'like_c...</td>\n",
       "      <td>[&lt;ReferencedTweet id=1623414925421944836 type=...</td>\n",
       "      <td>everyone</td>\n",
       "      <td>@HuJirui cenzuru implementuje i ChatGPT-obsahu...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>83544 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            author_id      conversation_id   \n",
       "0               0           4832549112  1624041287300046848  \\\n",
       "1               1  1390342845605695489  1624041268358807552   \n",
       "2               2  1224166856040173569  1624041263518425088   \n",
       "3               3  1531177796495888384  1624041262398537734   \n",
       "4               4            207164294  1624041258225106944   \n",
       "...           ...                  ...                  ...   \n",
       "83539          95           1184221022  1623742178194235396   \n",
       "83540          96            285659105  1623742176630034435   \n",
       "83541          97  1537579497725247488  1623742170078531586   \n",
       "83542          98   937660764428652545  1623742164831289346   \n",
       "83543          99  1453077305014767631  1623414925421944836   \n",
       "\n",
       "                      created_at edit_history_tweet_ids  geo   \n",
       "0      2023-02-10 13:43:10+00:00  [1624041287300046848]  NaN  \\\n",
       "1      2023-02-10 13:43:06+00:00  [1624041268358807552]  NaN   \n",
       "2      2023-02-10 13:43:05+00:00  [1624041263518425088]  NaN   \n",
       "3      2023-02-10 13:43:04+00:00  [1624041262398537734]  NaN   \n",
       "4      2023-02-10 13:43:03+00:00  [1624041258225106944]  NaN   \n",
       "...                          ...                    ...  ...   \n",
       "83539  2023-02-09 17:54:37+00:00  [1623742178194235396]  NaN   \n",
       "83540  2023-02-09 17:54:37+00:00  [1623742176630034435]  NaN   \n",
       "83541  2023-02-09 17:54:35+00:00  [1623742170078531586]  NaN   \n",
       "83542  2023-02-09 17:54:34+00:00  [1623742164831289346]  NaN   \n",
       "83543  2023-02-09 17:54:33+00:00  [1623742162050375687]  NaN   \n",
       "\n",
       "                        id  in_reply_to_user_id lang   \n",
       "0      1624041287300046848                  NaN   en  \\\n",
       "1      1624041268358807552                  NaN   th   \n",
       "2      1624041263518425088                  NaN   en   \n",
       "3      1624041262398537734                  NaN   en   \n",
       "4      1624041258225106944                  NaN   en   \n",
       "...                    ...                  ...  ...   \n",
       "83539  1623742178194235396                  NaN   es   \n",
       "83540  1623742176630034435                  NaN   en   \n",
       "83541  1623742170078531586                  NaN   en   \n",
       "83542  1623742164831289346                  NaN  zxx   \n",
       "83543  1623742162050375687         1.266613e+09   cs   \n",
       "\n",
       "                                          public_metrics   \n",
       "0      {'retweet_count': 264, 'reply_count': 0, 'like...  \\\n",
       "1      {'retweet_count': 8, 'reply_count': 0, 'like_c...   \n",
       "2      {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
       "3      {'retweet_count': 418, 'reply_count': 0, 'like...   \n",
       "4      {'retweet_count': 5, 'reply_count': 0, 'like_c...   \n",
       "...                                                  ...   \n",
       "83539  {'retweet_count': 6, 'reply_count': 1, 'like_c...   \n",
       "83540  {'retweet_count': 245, 'reply_count': 0, 'like...   \n",
       "83541  {'retweet_count': 1312, 'reply_count': 0, 'lik...   \n",
       "83542  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
       "83543  {'retweet_count': 0, 'reply_count': 0, 'like_c...   \n",
       "\n",
       "                                       referenced_tweets reply_settings   \n",
       "0      [<ReferencedTweet id=1623504190738690053 type=...       everyone  \\\n",
       "1      [<ReferencedTweet id=1624029463762866177 type=...       everyone   \n",
       "2                                                    NaN       everyone   \n",
       "3      [<ReferencedTweet id=1623685266480439298 type=...       everyone   \n",
       "4      [<ReferencedTweet id=1624028203307114496 type=...       everyone   \n",
       "...                                                  ...            ...   \n",
       "83539                                                NaN       everyone   \n",
       "83540  [<ReferencedTweet id=1623313196252827650 type=...       everyone   \n",
       "83541  [<ReferencedTweet id=1623577688513585152 type=...       everyone   \n",
       "83542                                                NaN       everyone   \n",
       "83543  [<ReferencedTweet id=1623414925421944836 type=...       everyone   \n",
       "\n",
       "                                                    text withheld  \n",
       "0      RT @sukeji_handa: The Delta Snake Review: A Mu...      NaN  \n",
       "1      RT @sxicex: ถึงเวลาของ Ai-Generated Music หลัง...      NaN  \n",
       "2      Why is there so much talk about a cyber attack...      NaN  \n",
       "3      RT @Connectome_CNTM: To celebrate the launch o...      NaN  \n",
       "4      RT @newscientist: ChatGPT can do more than jus...      NaN  \n",
       "...                                                  ...      ...  \n",
       "83539  ChatGPT cree los conceptos jurídicos de Lionel...      NaN  \n",
       "83540  RT @SrikantTFM: who needs ChatGPT AI when you ...      NaN  \n",
       "83541  RT @nocodedevs: ChatGPT is a FREE money-genera...      NaN  \n",
       "83542                            https://t.co/NlmZpb3wa5      NaN  \n",
       "83543  @HuJirui cenzuru implementuje i ChatGPT-obsahu...      NaN  \n",
       "\n",
       "[83544 rows x 14 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df0aabbf-18dd-4742-bdb9-555eeb52613b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 22:05:17 WARN Utils: Your hostname, abhijith-desktop resolves to a loopback address: 127.0.1.1; using 192.168.1.24 instead (on interface eth0)\n",
      "23/04/26 22:05:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/26 22:05:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/26 22:05:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9fbffaef-962b-41fe-9570-8b5f74e1d8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sdf = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9774ccbc-f166-4844-a632-8e336b996e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/26 14:20:21 WARN TaskSetManager: Stage 0 contains a task of very large size (6043 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+--------------------+----------------------+---+-------------------+-------------------+----+--------------------+--------------------+--------------+---------------------------------+--------+\n",
      "|Unnamed: 0|          author_id|    conversation_id|          created_at|edit_history_tweet_ids|geo|                 id|in_reply_to_user_id|lang|      public_metrics|   referenced_tweets|reply_settings|                             text|withheld|\n",
      "+----------+-------------------+-------------------+--------------------+----------------------+---+-------------------+-------------------+----+--------------------+--------------------+--------------+---------------------------------+--------+\n",
      "|        13|          221725492|1623980175573622784|2023-02-10 09:40:...|  [1623980175573622...|NaN|1623980175573622784|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @jathansadowsk...|     NaN|\n",
      "|        14|1613478477981970434|1623980171723419649|2023-02-10 09:40:...|  [1623980171723419...|NaN|1623980171723419649|                NaN|  zh|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @HitAshareLimi...|     NaN|\n",
      "|        15|           51101321|1623980170821369856|2023-02-10 09:40:...|  [1623980170821369...|NaN|1623980170821369856|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @garyblack00: ...|     NaN|\n",
      "|        16|           54606239|1623980170809077760|2023-02-10 09:40:...|  [1623980170809077...|NaN|1623980170809077760|                NaN|  ja|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @cicada3301_ki...|     NaN|\n",
      "|        17|          297503207|1623980161078116355|2023-02-10 09:40:...|  [1623980161078116...|NaN|1623980161078116355|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @Prathkum: Cha...|     NaN|\n",
      "|        18|1382357302494302209|1623980151439560704|2023-02-10 09:40:...|  [1623980151439560...|NaN|1623980151439560704|                NaN|  fr|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @Inafr_officie...|     NaN|\n",
      "|        19|1033481752164163584|1623980148755165184|2023-02-10 09:40:...|  [1623980148755165...|NaN|1623980148755165184|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @AndrewYNg: I ...|     NaN|\n",
      "|        20|1127312545243717632|1623980148490924032|2023-02-10 09:40:...|  [1623980148490924...|NaN|1623980148490924032|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             #Microsoft co-fou...|     NaN|\n",
      "|        21|            7739652|1623980144460288000|2023-02-10 09:40:...|  [1623980144460288...|NaN|1623980144460288000|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Google launches B...|     NaN|\n",
      "|        22|           60037279|1623980143957131264|2023-02-10 09:40:...|  [1623980143957131...|NaN|1623980143957131264|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Want to boost you...|     NaN|\n",
      "|        23|           28666886|1623762081328537601|2023-02-10 09:40:...|  [1623980135367032...|NaN|1623980135367032834|        3.2370736E7|  fr|{'retweet_count':...|[<ReferencedTweet...|      everyone|             @JeromeColombain ...|     NaN|\n",
      "|        24|          115799808|1623980133311971328|2023-02-10 09:40:...|  [1623980133311971...|NaN|1623980133311971328|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Opera may soon ge...|     NaN|\n",
      "|        25|1341692350347661313|1623980128765366274|2023-02-10 09:40:...|  [1623980128765366...|NaN|1623980128765366274|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Discover the pote...|     NaN|\n",
      "|        26|1487033236052135937|1623980127179751424|2023-02-10 09:40:...|  [1623980127179751...|NaN|1623980127179751424|                NaN|  de|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @jleinenbach: ...|     NaN|\n",
      "|        27|           28859710|1623980126554947589|2023-02-10 09:40:...|  [1623980126554947...|NaN|1623980126554947589|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Cybersecurity Cer...|     NaN|\n",
      "|        28|1582555870918893568|1623980126462681089|2023-02-10 09:40:...|  [1623980126462681...|NaN|1623980126462681089|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @AndrewYNg: I ...|     NaN|\n",
      "|        29|           53715488|1623980126072410112|2023-02-10 09:40:...|  [1623980126072410...|NaN|1623980126072410112|                NaN|  es|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @DotCSV: A ver...|     NaN|\n",
      "|        30|1618254483355766786|1623980118925545474|2023-02-10 09:40:...|  [1623980118925545...|NaN|1623980118925545474|                NaN|  zh|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @xiajingfa8: A...|     NaN|\n",
      "|        31|         1567441722|1623980115318263808|2023-02-10 09:40:...|  [1623980115318263...|NaN|1623980115318263808|                NaN|  de|{'retweet_count':...|                 NaN|      everyone|             🤔 Experiment: Üb...|     NaN|\n",
      "|        32|          110391990|1623980110083760128|2023-02-10 09:40:...|  [1623980110083760...|NaN|1623980110083760128|                NaN|  ja|{'retweet_count':...|                 NaN|      everyone|Metaは信頼がないのでしょうがな...|     NaN|\n",
      "+----------+-------------------+-------------------+--------------------+----------------------+---+-------------------+-------------------+----+--------------------+--------------------+--------------+---------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.write.parquet('td2.parquet',mode='overwrite')\n",
    "spark.read.parquet('td2.parquet').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ada6e1-bca9-465d-a060-15f13e9b4713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sdf  = spark.read.parquet('td2.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f307bad-db4b-47cc-bcbf-98d06602f611",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " Unnamed: 0             | 13                   \n",
      " author_id              | 221725492            \n",
      " conversation_id        | 1623980175573622784  \n",
      " created_at             | 2023-02-10 09:40:... \n",
      " edit_history_tweet_ids | [1623980175573622... \n",
      " geo                    | NaN                  \n",
      " id                     | 1623980175573622784  \n",
      " in_reply_to_user_id    | NaN                  \n",
      " lang                   | en                   \n",
      " public_metrics         | {'retweet_count':... \n",
      " referenced_tweets      | [<ReferencedTweet... \n",
      " reply_settings         | everyone             \n",
      " text                   | RT @jathansadowsk... \n",
      " withheld               | NaN                  \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f8f8a70-a7e3-4fa5-b546-d62ecf0f2177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_df = sdf.select((\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea351b46-20ae-4a32-a89e-269915908ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|                             text|\n",
      "+---------------------------------+\n",
      "|             RT @jathansadowsk...|\n",
      "|             RT @HitAshareLimi...|\n",
      "|             RT @garyblack00: ...|\n",
      "|             RT @cicada3301_ki...|\n",
      "|             RT @Prathkum: Cha...|\n",
      "|             RT @Inafr_officie...|\n",
      "|             RT @AndrewYNg: I ...|\n",
      "|             #Microsoft co-fou...|\n",
      "|             Google launches B...|\n",
      "|             Want to boost you...|\n",
      "|             @JeromeColombain ...|\n",
      "|             Opera may soon ge...|\n",
      "|             Discover the pote...|\n",
      "|             RT @jleinenbach: ...|\n",
      "|             Cybersecurity Cer...|\n",
      "|             RT @AndrewYNg: I ...|\n",
      "|             RT @DotCSV: A ver...|\n",
      "|             RT @xiajingfa8: A...|\n",
      "|             🤔 Experiment: Üb...|\n",
      "|Metaは信頼がないのでしょうがな...|\n",
      "+---------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7af26321-a69f-4b03-81c0-b0f269605621",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(text_df)\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "filtered_df = remover.transform(words_df)\n",
    "\n",
    "hashing_tf = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "tf_df = hashing_tf.transform(filtered_df)\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(tf_df)\n",
    "tfidf_df = idfModel.transform(tf_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135d3513-465f-4517-9288-c50f9d062131",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2735618391.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 18\u001b[0;36m\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"value\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\")\n",
    "stopwords_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\")\n",
    "lemma = LemmatizerModel.pretrained().setInputCols([\"cleanTokens\"]).setOutputCol(\"lemma\")\n",
    "finisher = Finisher().setInputCols([\"lemma\"]).setOutputCols([\"finished\"]).setOutputAsArray(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, normalizer, stopwords_cleaner, lemma, finisher, RegexTokenizer(inputCol=\"finished\", outputCol=\"tokens\", pattern=\"\\W\"), StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\"), HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000), IDF(inputCol=\"rawFeatures\", outputCol=\"features\"), LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb72a1a7-9dc9-4668-8d45-ed414f47d2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msparknlp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 9\u001b[0m document_assembler \u001b[38;5;241m=\u001b[39m \u001b[43mDocumentAssembler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m Normalizer()\u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/PySpark/pyspark/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PySpark/pyspark/lib/python3.10/site-packages/sparknlp/base/document_assembler.py:96\u001b[0m, in \u001b[0;36mDocumentAssembler.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDocumentAssembler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.DocumentAssembler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m, cleanupMode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/PySpark/pyspark/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PySpark/pyspark/lib/python3.10/site-packages/sparknlp/internal/annotator_transformer.py:36\u001b[0m, in \u001b[0;36mAnnotatorTransformer.__init__\u001b[0;34m(self, classname)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetParams(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PySpark/pyspark/lib/python3.10/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "document_assembler = DocumentAssembler().setInputCol(\"value\").setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\")\n",
    "stopwords_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\")\n",
    "lemma = LemmatizerModel.pretrained().setInputCols([\"cleanTokens\"]).setOutputCol(\"lemma\")\n",
    "finisher = Finisher().setInputCols([\"lemma\"]).setOutputCols([\"finished\"]).setOutputAsArray(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[document_assembler, tokenizer, normalizer, stopwords_cleaner, lemma, finisher, \n",
    "                            RegexTokenizer(inputCol=\"finished\", outputCol=\"tokens\", pattern=\"\\\\W\"), \n",
    "                            StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\"), \n",
    "                            HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=1000), \n",
    "                            IDF(inputCol=\"rawFeatures\", outputCol=\"features\"), \n",
    "                            LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca27397d-4ed3-47e1-97db-1c07c33780db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+--------------------+----------------------+---+-------------------+-------------------+----+--------------------+--------------------+--------------+---------------------------------+--------+\n",
      "|Unnamed: 0|          author_id|    conversation_id|          created_at|edit_history_tweet_ids|geo|                 id|in_reply_to_user_id|lang|      public_metrics|   referenced_tweets|reply_settings|                             text|withheld|\n",
      "+----------+-------------------+-------------------+--------------------+----------------------+---+-------------------+-------------------+----+--------------------+--------------------+--------------+---------------------------------+--------+\n",
      "|        13|          221725492|1623980175573622784|2023-02-10 09:40:...|  [1623980175573622...|NaN|1623980175573622784|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @jathansadowsk...|     NaN|\n",
      "|        14|1613478477981970434|1623980171723419649|2023-02-10 09:40:...|  [1623980171723419...|NaN|1623980171723419649|                NaN|  zh|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @HitAshareLimi...|     NaN|\n",
      "|        15|           51101321|1623980170821369856|2023-02-10 09:40:...|  [1623980170821369...|NaN|1623980170821369856|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @garyblack00: ...|     NaN|\n",
      "|        16|           54606239|1623980170809077760|2023-02-10 09:40:...|  [1623980170809077...|NaN|1623980170809077760|                NaN|  ja|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @cicada3301_ki...|     NaN|\n",
      "|        17|          297503207|1623980161078116355|2023-02-10 09:40:...|  [1623980161078116...|NaN|1623980161078116355|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @Prathkum: Cha...|     NaN|\n",
      "|        18|1382357302494302209|1623980151439560704|2023-02-10 09:40:...|  [1623980151439560...|NaN|1623980151439560704|                NaN|  fr|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @Inafr_officie...|     NaN|\n",
      "|        19|1033481752164163584|1623980148755165184|2023-02-10 09:40:...|  [1623980148755165...|NaN|1623980148755165184|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @AndrewYNg: I ...|     NaN|\n",
      "|        20|1127312545243717632|1623980148490924032|2023-02-10 09:40:...|  [1623980148490924...|NaN|1623980148490924032|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             #Microsoft co-fou...|     NaN|\n",
      "|        21|            7739652|1623980144460288000|2023-02-10 09:40:...|  [1623980144460288...|NaN|1623980144460288000|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Google launches B...|     NaN|\n",
      "|        22|           60037279|1623980143957131264|2023-02-10 09:40:...|  [1623980143957131...|NaN|1623980143957131264|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Want to boost you...|     NaN|\n",
      "|        23|           28666886|1623762081328537601|2023-02-10 09:40:...|  [1623980135367032...|NaN|1623980135367032834|        3.2370736E7|  fr|{'retweet_count':...|[<ReferencedTweet...|      everyone|             @JeromeColombain ...|     NaN|\n",
      "|        24|          115799808|1623980133311971328|2023-02-10 09:40:...|  [1623980133311971...|NaN|1623980133311971328|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Opera may soon ge...|     NaN|\n",
      "|        25|1341692350347661313|1623980128765366274|2023-02-10 09:40:...|  [1623980128765366...|NaN|1623980128765366274|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Discover the pote...|     NaN|\n",
      "|        26|1487033236052135937|1623980127179751424|2023-02-10 09:40:...|  [1623980127179751...|NaN|1623980127179751424|                NaN|  de|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @jleinenbach: ...|     NaN|\n",
      "|        27|           28859710|1623980126554947589|2023-02-10 09:40:...|  [1623980126554947...|NaN|1623980126554947589|                NaN|  en|{'retweet_count':...|                 NaN|      everyone|             Cybersecurity Cer...|     NaN|\n",
      "|        28|1582555870918893568|1623980126462681089|2023-02-10 09:40:...|  [1623980126462681...|NaN|1623980126462681089|                NaN|  en|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @AndrewYNg: I ...|     NaN|\n",
      "|        29|           53715488|1623980126072410112|2023-02-10 09:40:...|  [1623980126072410...|NaN|1623980126072410112|                NaN|  es|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @DotCSV: A ver...|     NaN|\n",
      "|        30|1618254483355766786|1623980118925545474|2023-02-10 09:40:...|  [1623980118925545...|NaN|1623980118925545474|                NaN|  zh|{'retweet_count':...|[<ReferencedTweet...|      everyone|             RT @xiajingfa8: A...|     NaN|\n",
      "|        31|         1567441722|1623980115318263808|2023-02-10 09:40:...|  [1623980115318263...|NaN|1623980115318263808|                NaN|  de|{'retweet_count':...|                 NaN|      everyone|             🤔 Experiment: Üb...|     NaN|\n",
      "|        32|          110391990|1623980110083760128|2023-02-10 09:40:...|  [1623980110083760...|NaN|1623980110083760128|                NaN|  ja|{'retweet_count':...|                 NaN|      everyone|Metaは信頼がないのでしょうがな...|     NaN|\n",
      "+----------+-------------------+-------------------+--------------------+----------------------+---+-------------------+-------------------+----+--------------------+--------------------+--------------+---------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2dcb1a09-53f3-4340-9baf-d5bd0ff8cb4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_tweets = sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9e83049-ebba-42f7-9712-428260a5b4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                                                                                                                            |processed_text                                                                                                                                                                                                |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|RT @jathansadowski: As much as Ted Chiang's fiction is fantastic, it's really his non-fiction that I could read forever. His two paragraphs…                                                                                    |  As much as Ted Chiang s fiction is fantastic  it s really his non fiction that I could read forever  His two paragraphs…                                                                                    |\n",
      "|RT @HitAshareLimit: 0210号 A股复盘文章 #观点   #chatGPT\\nhttps://t.co/Q8YpYqwsVp\\nA股最全面的复盘文章，赶紧关注，每天送吃肉好票！\\n\\n免费的电报群在主页，想进来一起学习交流的，点击链接即可进入https://t.co…                    |  号 A股复盘文章 #观点   #chatGPT\\n\\nA股最全面的复盘文章，赶紧关注，每天送吃肉好票！\\n\\n免费的电报群在主页，想进来一起学习交流的，点击链接即可进入                                                            |\n",
      "|RT @garyblack00: Investors are bats—t crazy if they think new Bing with ChatGPT is going to get consumers to abandon $GOOGL search in favor…                                                                                    |  Investors are bats—t crazy if they think new Bing with ChatGPT is going to get consumers to abandon  GOOGL search in favor…                                                                                 |\n",
      "|RT @cicada3301_kig: ChatGPTで我々がおそらく勘違いしてるのは、我々の思考が dual process theory で動いていて「アホみたいな内容の無意識な連想」の上に「学術的な理詰め」を乗せている二重構造なのに、ChatGPTは我々より連想力が遥かに…|  ChatGPTで我々がおそらく勘違いしてるのは、我々の思考が dual process theory で動いていて「アホみたいな内容の無意識な連想」の上に「学術的な理詰め」を乗せている二重構造なのに、ChatGPTは我々より連想力が遥かに…|\n",
      "|RT @Prathkum: ChatGPT makes you efficient.\\n\\nAI resume builder get you a job:\\n\\n1. https://t.co/eut8xxqYDq\\nAI resume writing assistant\\n\\n2. ht…                                                                             |  ChatGPT makes you efficient \\n\\nAI resume builder get you a job \\n\\n  \\nAI resume writing assistant\\n\\n  ht…                                                                                                |\n",
      "|RT @Inafr_officiel: Ce soir, dans l'émission Inattendu, on revient sur la technologie ChatGPT. Rendez-vous à 21h20 sur Franceinfo (Canal 27…                                                                                    |  Ce soir  dans l émission Inattendu  on revient sur la technologie ChatGPT  Rendez vous à h sur Franceinfo  Canal …                                                                                          |\n",
      "|RT @AndrewYNg: I wish schools could make homework so joyful that students want to do it themselves, rather than let ChatGPT have all the fu…                                                                                    |  I wish schools could make homework so joyful that students want to do it themselves  rather than let ChatGPT have all the fu…                                                                               |\n",
      "|#Microsoft co-founder #BillGates: #ChatGPT 'will change our world' https://t.co/YNtP5ma3uP                                                                                                                                      |#Microsoft co founder #BillGates  #ChatGPT  will change our world                                                                                                                                             |\n",
      "|Google launches Bard, its answer to ChatGPT – here’s what it looks like https://t.co/29ocJXuz73 #Google                                                                                                                         |Google launches Bard  its answer to ChatGPT – here’s what it looks like  #Google                                                                                                                              |\n",
      "|Want to boost your productivity and effectiveness? Follow my #Linkedin Newsletter on #ChatGPT for tips and tricks! https://t.co/AA4vX76WG8 #Productivity #personalgrowth                                                        |Want to boost your productivity and effectiveness  Follow my #Linkedin Newsletter on #ChatGPT for tips and tricks   #Productivity #personalgrowth                                                             |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 54\u001b[0m\n\u001b[1;32m     45\u001b[0m raw_tweets\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Create a tokenizer that Filter away tokens with length < 3, and get rid of symbols like $,#,...\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(3).setInputCol(\"processed_text\").setOutputCol(\"tokens\")\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# perform lemmatization using wordNetLemmatizer\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m     55\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    " # Preprocess steps\n",
    "import re\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.sql.functions import split, explode, udf, lit\n",
    "\n",
    "# convert create_at column to date\n",
    "raw_tweets=raw_tweets.withColumn(\"created_date\", raw_tweets['created_at'].cast(T.DateType()))\n",
    "\n",
    "# define process function\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@â'\n",
    "def remove_links(tweet):\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) \n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) \n",
    "    tweet = tweet.strip('[link]') \n",
    "    return tweet\n",
    "def remove_users(tweet):\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet\n",
    "def remove_punctuation(tweet):\n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) \n",
    "    return tweet\n",
    "def remove_number(tweet):\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) \n",
    "    return tweet\n",
    "def remove_hashtag(tweet):\n",
    "    tweet = re.sub('(#[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) \n",
    "    return tweet\n",
    "    \n",
    "# register user defined function\n",
    "remove_links=udf(remove_links)\n",
    "remove_users=udf(remove_users)\n",
    "remove_punctuation=udf(remove_punctuation)\n",
    "remove_number=udf(remove_number)\n",
    "remove_hashtag=udf(remove_hashtag)\n",
    "\n",
    "raw_tweets=raw_tweets.withColumn('processed_text', remove_links(raw_tweets['text']))\n",
    "raw_tweets=raw_tweets.withColumn('processed_text', remove_users(raw_tweets['processed_text']))\n",
    "raw_tweets=raw_tweets.withColumn('processed_text', remove_punctuation(raw_tweets['processed_text']))\n",
    "raw_tweets=raw_tweets.withColumn('processed_text', remove_number(raw_tweets['processed_text']))\n",
    "\n",
    "# show the data before and after pocessed\n",
    "raw_tweets.select('text','processed_text').show(10, False)\n",
    "\n",
    "# Create a tokenizer that Filter away tokens with length < 3, and get rid of symbols like $,#,...\n",
    "# tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(3).setInputCol(\"processed_text\").setOutputCol(\"tokens\")\n",
    "\n",
    "# Tokenize tweets\n",
    "# tokenized_tweets = tokenizer.transform(raw_tweets)\n",
    "\n",
    "# perform lemmatization using wordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#from nltk.stem.lancaster import LancasterStemmer\n",
    "#from nltk import SnowballStemmer\n",
    "#from nltk.stem import PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#st = LancasterStemmer()\n",
    "#stemmer = SnowballStemmer(\"english\")\n",
    "#stemmer=PorterStemmer()\n",
    "\n",
    "def lemmatization(row):\n",
    "    #row = [stemmer.stem(lemmatizer.lemmatize(word)) for word in row]\n",
    "    row = [lemmatizer.lemmatize(word,'v') for word in row]\n",
    "    return row\n",
    "\n",
    "lemmatization = udf(lemmatization)\n",
    "tokenized_tweets=tokenized_tweets.withColumn('tokens_lemma', lemmatization(tokenized_tweets['tokens']))\n",
    "\n",
    "# create cutomized extended stop word list\n",
    "stopwordList = [\"singapore\",\"Singapore\"]\n",
    "StopWordsRemover().getStopWords()\n",
    "stopwordList.extend(StopWordsRemover().getStopWords())\n",
    "stopwordList = list(set(stopwordList))\n",
    "\n",
    "# reference: https://sites.google.com/site/iamgongwei/home/sw\n",
    "# this is a twitter specific stop word summarized by SMU researcher\n",
    "# to add more common stopwords,especially in twitter context\n",
    "f_twitter_stop_words = open('twitter-stopwords - TA - Less.txt','r')\n",
    "f_twitter_stop_words_content = f_twitter_stop_words.read()\n",
    "#print(content)\n",
    "twitter_stopwords = f_twitter_stop_words_content.split(\",\")\n",
    "twitter_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5e56c7e-4902-4774-97f2-26e31a35f3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sparknlp.base import *\n",
    "# from sparknlp.annotator import *\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql.types import *\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a51ad6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m regexTokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRegexTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39msetInputCols([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregexToken\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39msetToLowercase(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/sparknlp/annotator/token/regex_tokenizer.py:90\u001b[0m, in \u001b[0;36mRegexTokenizer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRegexTokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.annotators.RegexTokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(\n\u001b[1;32m     92\u001b[0m         inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     93\u001b[0m         outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregexToken\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m         preservePosition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/sparknlp/common/annotator_model.py:37\u001b[0m, in \u001b[0;36mAnnotatorModel.__init__\u001b[0;34m(self, classname, java_model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classname \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m java_model:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m java_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_from_java()\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "regexTokenizer = RegexTokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"regexToken\") \\\n",
    "    .setToLowercase(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fdcb7029-6ebc-4278-9d59-3388902a951c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mRegexTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msetPattern(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mW_]+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMinTokenLength(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39msetInputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetOutputCol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/sparknlp/annotator/token/regex_tokenizer.py:90\u001b[0m, in \u001b[0;36mRegexTokenizer.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;129m@keyword_only\u001b[39m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mRegexTokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcom.johnsnowlabs.nlp.annotators.RegexTokenizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setDefault(\n\u001b[1;32m     92\u001b[0m         inputCols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     93\u001b[0m         outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregexToken\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m         preservePosition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/pyspark/__init__.py:135\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/sparknlp/common/annotator_model.py:37\u001b[0m, in \u001b[0;36mAnnotatorModel.__init__\u001b[0;34m(self, classname, java_model)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classname \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m java_model:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m_java_class_name \u001b[38;5;241m=\u001b[39m classname\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_java_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m java_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_from_java()\n",
      "File \u001b[0;32m~/Desktop/PySpark/pysparktw/lib/python3.10/site-packages/pyspark/ml/wrapper.py:86\u001b[0m, in \u001b[0;36mJavaWrapper._new_java_obj\u001b[0;34m(java_class, *args)\u001b[0m\n\u001b[1;32m     84\u001b[0m     java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(java_obj, name)\n\u001b[1;32m     85\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjava_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer().setPattern(\"[\\\\W_]+\").setMinTokenLength(3).setInputCol(\"processed_text\").setOutputCol(\"tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eab9d6cb-492a-4110-a171-96ce49339204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.1\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "print(sparknlp.version())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7355588a-99a9-4076-9659-2034672a75bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "23/04/26 22:20:29 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34feda05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
